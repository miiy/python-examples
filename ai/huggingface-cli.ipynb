{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMJuvaxEtnelao7PJ6nhNH3"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## huggingface-cli\n",
    "\n",
    "<https://huggingface.co/docs/huggingface_hub/main/en/guides/cli>\n",
    "\n",
    "\n",
    "使用 git-lfs 下载模型\n",
    "\n",
    "<https://huggingface.co/docs/hub/models-downloading>\n",
    "\n",
    "\n",
    "```bash\n",
    "sudo apt install git-lfs\n",
    "git lfs install\n",
    "git clone https://huggingface.co/distilgpt2\n",
    "```\n",
    "\n",
    "使用 huggingface-cli"
   ],
   "metadata": {
    "id": "HTGuyB2h0Sim"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DRQ_2f3iVwz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704794098450,
     "user_tz": -480,
     "elapsed": 9565,
     "user": {
      "displayName": "felix",
      "userId": "09327544258569883899"
     }
    },
    "outputId": "23cb7f15-9b10-48c9-8bc0-318ad8c76a19"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
      "Collecting huggingface_hub[cli]\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m330.3/330.3 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (23.2)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.7/67.7 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2023.11.17)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.12)\n",
      "Installing collected packages: pfzy, InquirerPy, huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.20.1\n",
      "    Uninstalling huggingface-hub-0.20.1:\n",
      "      Successfully uninstalled huggingface-hub-0.20.1\n",
      "Successfully installed InquirerPy-0.3.4 huggingface_hub-0.20.2 pfzy-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!huggingface-cli download --help"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrOgb5FSinFO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704792363900,
     "user_tz": -480,
     "elapsed": 647,
     "user": {
      "displayName": "felix",
      "userId": "09327544258569883899"
     }
    },
    "outputId": "427c7ad6-94b3-420b-b3fc-574ab2f36a9d"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "usage: huggingface-cli <command> [<args>] download [-h] [--repo-type {model,dataset,space}]\n",
      "                                                   [--revision REVISION] [--include [INCLUDE ...]]\n",
      "                                                   [--exclude [EXCLUDE ...]]\n",
      "                                                   [--cache-dir CACHE_DIR] [--local-dir LOCAL_DIR]\n",
      "                                                   [--local-dir-use-symlinks {auto,True,False}]\n",
      "                                                   [--force-download] [--resume-download]\n",
      "                                                   [--token TOKEN] [--quiet]\n",
      "                                                   repo_id [filenames ...]\n",
      "\n",
      "positional arguments:\n",
      "  repo_id               ID of the repo to download from (e.g. `username/repo-name`).\n",
      "  filenames             Files to download (e.g. `config.json`, `data/metadata.jsonl`).\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --repo-type {model,dataset,space}\n",
      "                        Type of repo to download from (e.g. `dataset`).\n",
      "  --revision REVISION   An optional Git revision id which can be a branch name, a tag, or a commit\n",
      "                        hash.\n",
      "  --include [INCLUDE ...]\n",
      "                        Glob patterns to match files to download.\n",
      "  --exclude [EXCLUDE ...]\n",
      "                        Glob patterns to exclude from files to download.\n",
      "  --cache-dir CACHE_DIR\n",
      "                        Path to the directory where to save the downloaded files.\n",
      "  --local-dir LOCAL_DIR\n",
      "                        If set, the downloaded file will be placed under this directory either as\n",
      "                        a symlink (default) or a regular file. Check out\n",
      "                        https://huggingface.co/docs/huggingface_hub/guides/download#download-\n",
      "                        files-to-local-folder for more details.\n",
      "  --local-dir-use-symlinks {auto,True,False}\n",
      "                        To be used with `local_dir`. If set to 'auto', the cache directory will be\n",
      "                        used and the file will be either duplicated or symlinked to the local\n",
      "                        directory depending on its size. It set to `True`, a symlink will be\n",
      "                        created, no matter the file size. If set to `False`, the file will either\n",
      "                        be duplicated from cache (if already exists) or downloaded from the Hub\n",
      "                        and not cached.\n",
      "  --force-download      If True, the files will be downloaded even if they are already cached.\n",
      "  --resume-download     If True, resume a previously interrupted download.\n",
      "  --token TOKEN         A User Access Token generated from https://huggingface.co/settings/tokens\n",
      "  --quiet               If True, progress bars are disabled and only the path to the download\n",
      "                        files is printed.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!huggingface-cli download THUDM/chatglm3-6b\n",
    "# 指定本地文件夹，从cache软链接到本地文件夹\n",
    "# !huggingface-cli download THUDM/chatglm3-6b --local-dir /content/chatglm3-6b/\n",
    "# 指定本地文件夹\n",
    "# !huggingface-cli download THUDM/chatglm3-6b --local-dir /home/user/huggingface/models/chatglm3-6b/ --local-dir-use-symlinks=False"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMolfLZ7j-1M",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704793120657,
     "user_tz": -480,
     "elapsed": 2290,
     "user": {
      "displayName": "felix",
      "userId": "09327544258569883899"
     }
    },
    "outputId": "c0728f3d-439a-474f-8c21-f6392c170b3d"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "Fetching 26 files: 100% 26/26 [00:01<00:00, 24.63it/s]\n",
      "/root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/b098244a71fbe69ce149682d9072a7629f7e908c\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 扫描缓存\n",
    "!huggingface-cli scan-cache"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qteZZBtalpKU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704793266324,
     "user_tz": -480,
     "elapsed": 873,
     "user": {
      "displayName": "felix",
      "userId": "09327544258569883899"
     }
    },
    "outputId": "c2ad9c6c-5e21-4b91-abbc-2d524508b031"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REPO ID           REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED LAST_MODIFIED REFS LOCAL PATH                                              \n",
      "----------------- --------- ------------ -------- ------------- ------------- ---- ------------------------------------------------------- \n",
      "THUDM/chatglm3-6b model            25.0G       26 8 minutes ago 6 minutes ago main /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b \n",
      "\n",
      "Done in 0.0s. Scanned 1 repo(s) for a total of \u001B[1m\u001B[31m25.0G\u001B[0m.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!ls -alh /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/b098244a71fbe69ce149682d9072a7629f7e908c\n",
    "!ls -alh /home/user/huggingface/models/chatglm3-6b/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1G9bfB6tmNF3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704796939449,
     "user_tz": -480,
     "elapsed": 357,
     "user": {
      "displayName": "felix",
      "userId": "09327544258569883899"
     }
    },
    "outputId": "339a0193-70d4-42f9-98bf-b230a6c59d90"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 8.0K\n",
      "drwxr-xr-x 2 root root 4.0K Jan  9 09:55 .\n",
      "drwxr-xr-x 3 root root 4.0K Jan  9 09:55 ..\n",
      "total 24G\n",
      "drwxr-xr-x 2 root root  4.0K Jan  9 09:57 .\n",
      "drwxr-xr-x 3 root root  4.0K Jan  9 09:57 ..\n",
      "-rw-r--r-- 1 root root  1.3K Jan  9 09:55 config.json\n",
      "-rw-r--r-- 1 root root  2.3K Jan  9 09:55 configuration_chatglm.py\n",
      "-rw-r--r-- 1 root root  1.5K Jan  9 09:55 .gitattributes\n",
      "-rw-r--r-- 1 root root  1.8G Jan  9 09:56 model-00001-of-00007.safetensors\n",
      "-rw-r--r-- 1 root root  1.9G Jan  9 09:56 model-00002-of-00007.safetensors\n",
      "-rw-r--r-- 1 root root  1.8G Jan  9 09:56 model-00003-of-00007.safetensors\n",
      "-rw-r--r-- 1 root root  1.7G Jan  9 09:56 model-00004-of-00007.safetensors\n",
      "-rw-r--r-- 1 root root  1.9G Jan  9 09:56 model-00005-of-00007.safetensors\n",
      "-rw-r--r-- 1 root root  1.8G Jan  9 09:56 model-00006-of-00007.safetensors\n",
      "-rw-r--r-- 1 root root 1005M Jan  9 09:56 model-00007-of-00007.safetensors\n",
      "-rw-r--r-- 1 root root   55K Jan  9 09:55 modeling_chatglm.py\n",
      "-rw-r--r-- 1 root root  4.1K Jan  9 09:55 MODEL_LICENSE\n",
      "-rw-r--r-- 1 root root   21K Jan  9 09:55 model.safetensors.index.json\n",
      "-rw-r--r-- 1 root root  1.8G Jan  9 09:56 pytorch_model-00001-of-00007.bin\n",
      "-rw-r--r-- 1 root root  1.9G Jan  9 09:57 pytorch_model-00002-of-00007.bin\n",
      "-rw-r--r-- 1 root root  1.8G Jan  9 09:57 pytorch_model-00003-of-00007.bin\n",
      "-rw-r--r-- 1 root root  1.7G Jan  9 09:57 pytorch_model-00004-of-00007.bin\n",
      "-rw-r--r-- 1 root root  1.9G Jan  9 09:57 pytorch_model-00005-of-00007.bin\n",
      "-rw-r--r-- 1 root root  1.8G Jan  9 09:57 pytorch_model-00006-of-00007.bin\n",
      "-rw-r--r-- 1 root root 1005M Jan  9 09:57 pytorch_model-00007-of-00007.bin\n",
      "-rw-r--r-- 1 root root   20K Jan  9 09:56 pytorch_model.bin.index.json\n",
      "-rw-r--r-- 1 root root   15K Jan  9 09:56 quantization.py\n",
      "-rw-r--r-- 1 root root  7.3K Jan  9 09:55 README.md\n",
      "-rw-r--r-- 1 root root   13K Jan  9 09:56 tokenization_chatglm.py\n",
      "-rw-r--r-- 1 root root   518 Jan  9 09:56 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root  995K Jan  9 09:56 tokenizer.model\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 打印有关机器设置的详细信息\n",
    "!huggingface-cli env"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jLj0K6tneRd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704796724719,
     "user_tz": -480,
     "elapsed": 519,
     "user": {
      "displayName": "felix",
      "userId": "09327544258569883899"
     }
    },
    "outputId": "9ad695cb-6319-41d7-b8e1-6a557fd3c91c"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Copy-and-paste the text below in your GitHub issue.\n",
      "\n",
      "- huggingface_hub version: 0.20.2\n",
      "- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\n",
      "- Python version: 3.10.12\n",
      "- Running in iPython ?: No\n",
      "- Running in notebook ?: No\n",
      "- Running in Google Colab ?: No\n",
      "- Token path ?: /root/.cache/huggingface/token\n",
      "- Has saved token ?: False\n",
      "- Configured git credential helpers: \n",
      "- FastAI: 2.7.13\n",
      "- Tensorflow: 2.15.0\n",
      "- Torch: 2.1.0+cu121\n",
      "- Jinja2: 3.1.2\n",
      "- Graphviz: 0.20.1\n",
      "- Pydot: 1.4.2\n",
      "- Pillow: 9.4.0\n",
      "- hf_transfer: N/A\n",
      "- gradio: N/A\n",
      "- tensorboard: N/A\n",
      "- numpy: 1.23.5\n",
      "- pydantic: 1.10.13\n",
      "- aiohttp: 3.9.1\n",
      "- ENDPOINT: https://huggingface.co\n",
      "- HF_HUB_CACHE: /root/.cache/huggingface/hub\n",
      "- HF_ASSETS_CACHE: /root/.cache/huggingface/assets\n",
      "- HF_TOKEN_PATH: /root/.cache/huggingface/token\n",
      "- HF_HUB_OFFLINE: False\n",
      "- HF_HUB_DISABLE_TELEMETRY: False\n",
      "- HF_HUB_DISABLE_PROGRESS_BARS: None\n",
      "- HF_HUB_DISABLE_SYMLINKS_WARNING: False\n",
      "- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\n",
      "- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\n",
      "- HF_HUB_ENABLE_HF_TRANSFER: False\n",
      "- HF_HUB_ETAG_TIMEOUT: 10\n",
      "- HF_HUB_DOWNLOAD_TIMEOUT: 10\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "czwj1-TqzZh5"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
